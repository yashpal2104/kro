// Copyright 2025 The Kubernetes Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Package dynamiccontroller provides a flexible and efficient solution for
// managing multiple GroupVersionResources (GVRs) in a Kubernetes environment.
// It implements a single controller capable of dynamically handling various
// resource types concurrently, adapting to runtime changes without system restarts.
//
// Key features and design considerations:
//
//  1. Multi GVR management: It handles multiple resource types concurrently,
//     creating and managing separate workflows for each.
//
//  2. Dynamic informer management: Creates and deletes informers on the fly
//     for new resource types, allowing real time adaptation to changes in the
//     cluster.
//
//  3. Minimal disruption: Operations on one resource type do not affect
//     the performance or functionality of others.
//
//  4. Minimalism: Unlike controller-runtime, this implementation
//     is tailored specifically for kro's needs, avoiding unnecessary
//     dependencies and overhead.
//
//  5. Future Extensibility: It allows for future enhancements such as
//     sharding and CEL cost aware leader election, which are not readily
//     achievable with k8s.io/controller-runtime.
//
// Why not use k8s.io/controller-runtime:
//
//  1. Static nature: controller-runtime is optimized for statically defined
//     controllers, however kro requires runtime creation and management
//     of controllers for various GVRs.
//
//  2. Overhead reduction: by not including unused features like leader election
//     and certain metrics, this implementation remains minimalistic and efficient.
//
//  3. Customization: this design allows for deep customization and
//     optimization specific to kro's unique requirements for managing
//     multiple GVRs dynamically.
//
// This implementation aims to provide a reusable, efficient, and flexible
// solution for dynamic multi-GVR controller management in Kubernetes environments.
//
// NOTE(a-hilaly): Potentially we might open source this package for broader use cases.
package dynamiccontroller

import (
	"context"
	"fmt"
	"strings"
	"sync"
	"time"

	"github.com/go-logr/logr"
	"golang.org/x/time/rate"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/api/meta"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/apimachinery/pkg/types"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	"k8s.io/apimachinery/pkg/util/wait"
	k8smetadata "k8s.io/client-go/metadata"
	"k8s.io/client-go/tools/cache"
	"k8s.io/client-go/util/workqueue"
	ctrl "sigs.k8s.io/controller-runtime"

	"github.com/kubernetes-sigs/kro/pkg/dynamiccontroller/internal"
	"github.com/kubernetes-sigs/kro/pkg/metadata"
	"github.com/kubernetes-sigs/kro/pkg/requeue"
)

const (
	// eventTypeAdd is emitted to the queue when a new object was created
	eventTypeAdd = "add"
	// eventTypeUpdate is emitted to the queue when an existing object was updated
	eventTypeUpdate = "update"
	// eventTypeDelete is emitted to the queue when an existing object was deleted
	eventTypeDelete = "delete"
)

// Config holds the configuration for DynamicController
type Config struct {
	// Workers specifies the number of workers processing items from the queue
	Workers int
	// ResyncPeriod defines the interval at which the controller will re list
	// the resources, even if there haven't been any changes.
	ResyncPeriod time.Duration
	// QueueMaxRetries is the maximum number of retries for an item in the queue
	// will be retried before being dropped.
	//
	// NOTE(a-hilaly): I'm not very sure how useful is this, i'm trying to avoid
	// situations where reconcile errors exhaust the queue.
	QueueMaxRetries int
	// MinRetryDelay is the minimum delay before retrying an item in the queue
	MinRetryDelay time.Duration
	// MaxRetryDelay is the maximum delay before retrying an item in the queue
	MaxRetryDelay time.Duration
	// RateLimit is the maximum number of events processed per second
	RateLimit int
	// BurstLimit is the maximum number of events in a burst
	BurstLimit int
	// QueueShutdownTimeout is the maximum time to wait for the queue to drain before shutting down.
	QueueShutdownTimeout time.Duration
}

// Handler is used to actually perform the reconciliation logic for an instance GVR and will operate
// on a single instance of the resource received from the queue
type Handler func(ctx context.Context, req ctrl.Request) error

// ObjectIdentifiers holds the key and GVR of the object to reconcile.
type ObjectIdentifiers struct {
	types.NamespacedName
	GVR schema.GroupVersionResource
}

// registration tracks one parent GVR registration and its child handler IDs.
// Each parent may own one "parent handler" and multiple "child handlers".
type registration struct {
	parentGVR schema.GroupVersionResource

	parentHandlerID string
	childHandlerIDs map[schema.GroupVersionResource]string
}

// DynamicController (DC) is a single controller capable of managing multiple different
// kubernetes resources (GVRs) in parallel. It can safely start watching new
// resources and stop watching others at runtime - hence the term "dynamic". This
// flexibility allows us to accept and manage various resources in a Kubernetes
// cluster without requiring restarts or pod redeployments.
//
// It is mainly inspired by native Kubernetes controllers but designed for more
// flexible and lightweight operation. DC serves as the core component of kro's
// dynamic resource management system. Its primary purpose is to create and manage
// "micro" controllers for custom resources defined by users at runtime (via the
// ResourceGraphDefinition CRs).
//
// DynamicController manages all handlers and informers.
// It uses two levels of locking:
//
//  1. dc.mu protects *global maps* (watches and registrations).
//     This ensures consistency when adding/removing GVRs or
//     attaching/detaching children.
//
//  2. Each perGVRWatch has its own w.mu to protect *per-informer state*
//     (handler map). This allows concurrent updates to
//     different GVRs without blocking each other.
type DynamicController struct {
	// Parent run context, inherited by all informer stop contexts.
	// It is set by Start and meant to be used to register the controller context with a global handler
	// such as the controller-runtime manager.
	ctx context.Context

	// mu is a global mutex protecting watches and registrations.
	// Required because StartServingGVK and StopServiceGVK may run concurrently,
	// and because Run or gracefulShutdown may also traverse these maps.
	mu sync.Mutex

	// config is the controller configuration used to steer detailed enqueue behavior when events
	// are received by the dynamic controllers watches and need to be propagated to the handlers.
	config Config

	log logr.Logger

	// client is the Kubernetes client used to create informers and list resources.
	// DynamicController only operates on metadata information, because all actual reconciliations
	// happen in the instance handlers that are managed with Register and Deregister.
	client k8smetadata.Interface
	// mapper is used to translate from GVRs (REST Resources that can be watched) to GVKs to properly
	// propagate instance events to the handlers.
	mapper meta.RESTMapper

	// Map of active informers per GVR.
	// No matter if an instance resource or a child resource is being watched,
	// eventually, they will be handled by an internal.LazyInformer registered here.
	// Guarded by mu.
	watches map[schema.GroupVersionResource]*internal.LazyInformer
	// Map of parent registrations per GVR.
	// A registration is created for each parent GVR that is being watched.
	// Each registration contains a list of child GVRs that are being watched for the parent.
	// Any event that is received for a child GVR will be propagated through a labelled reference to the parent.
	// Guarded by mu.
	registrations map[schema.GroupVersionResource]*registration

	// handlers is a Handler collection for each parent GVR, invoked for queued objects.
	handlers sync.Map // map[schema.GroupVersionResource]Handler (thread-safe on its own)
	// queue is the work queue used to process items received via watches.
	// The queue is shared between all informers and is used to propagate events to the handlers.
	queue workqueue.TypedRateLimitingInterface[ObjectIdentifiers]
}

// NewDynamicController creates a new DynamicController.
func NewDynamicController(
	log logr.Logger,
	config Config,
	kubeClient k8smetadata.Interface,
	mapper meta.RESTMapper,
) *DynamicController {
	logger := log.WithName("dynamic-controller")

	return &DynamicController{
		config:        config,
		log:           logger,
		client:        kubeClient,
		mapper:        mapper,
		watches:       make(map[schema.GroupVersionResource]*internal.LazyInformer),
		registrations: make(map[schema.GroupVersionResource]*registration),
		queue: workqueue.NewTypedRateLimitingQueueWithConfig(workqueue.NewTypedMaxOfRateLimiter(
			workqueue.NewTypedItemExponentialFailureRateLimiter[ObjectIdentifiers](config.MinRetryDelay, config.MaxRetryDelay),
			&workqueue.TypedBucketRateLimiter[ObjectIdentifiers]{Limiter: rate.NewLimiter(rate.Limit(config.RateLimit), config.BurstLimit)},
		), workqueue.TypedRateLimitingQueueConfig[ObjectIdentifiers]{Name: "dynamic-controller-queue"}),
	}
}

// Start starts workers and blocks until ctx.Done().
func (dc *DynamicController) Start(ctx context.Context) error {
	if dc.ctx != nil {
		return fmt.Errorf("already running")
	}

	defer utilruntime.HandleCrash()

	dc.log.Info("Starting dynamic controller")
	defer dc.log.Info("Shutting down dynamic controller")

	dc.ctx = ctx

	// Workers.
	for i := 0; i < dc.config.Workers; i++ {
		go wait.UntilWithContext(ctx, dc.worker, time.Second)
	}

	<-ctx.Done()
	return dc.gracefulShutdown()
}

func (dc *DynamicController) worker(ctx context.Context) {
	for dc.processNextWorkItem(ctx) {
	}
}

func (dc *DynamicController) processNextWorkItem(ctx context.Context) bool {
	item, shutdown := dc.queue.Get()
	if shutdown {
		return false
	}
	defer dc.queue.Done(item)

	// metric: queueLength
	queueLength.Set(float64(dc.queue.Len()))

	handler, ok := dc.handlers.Load(item.GVR)
	if !ok {
		// this can happen if the handler was removed and we still have items in flight in the queue.
		dc.log.V(1).Info("handler for gvr no longer exists, dropping item", "item", item)
		dc.queue.Forget(item)
		return true
	}

	err := dc.syncFunc(ctx, item, handler.(Handler))
	if err == nil {
		dc.queue.Forget(item)
		return true
	}

	gvrKey := keyFromGVR(item.GVR)

	switch typedErr := err.(type) {
	case *requeue.NoRequeue:
		dc.log.Error(typedErr, "Error syncing item, not requeuing", "item", item)
		requeueTotal.WithLabelValues(gvrKey, "no_requeue").Inc()
		dc.queue.Forget(item)
	case *requeue.RequeueNeeded:
		dc.log.V(1).Info("Requeue needed", "item", item, "error", typedErr)
		requeueTotal.WithLabelValues(gvrKey, "requeue").Inc()
		dc.queue.Add(item)
	case *requeue.RequeueNeededAfter:
		dc.log.V(1).Info("Requeue needed after delay", "item", item, "error", typedErr, "delay", typedErr.Duration())
		requeueTotal.WithLabelValues(gvrKey, "requeue_after").Inc()
		dc.queue.AddAfter(item, typedErr.Duration())
	default:
		// we only check here for this not found error here because we want explicit requeue signals to have priority
		if apierrors.IsNotFound(err) {
			dc.log.V(1).Info("item no longer exists, dropping from queue", "item", item)
			dc.queue.Forget(item)
			return true
		}
		requeueTotal.WithLabelValues(gvrKey, "rate_limited").Inc()
		if dc.queue.NumRequeues(item) < dc.config.QueueMaxRetries {
			dc.log.Error(err, "Error syncing item, requeuing with rate limit", "item", item)
			dc.queue.AddRateLimited(item)
		} else {
			dc.log.Error(err, "Dropping item from queue after max retries", "item", item)
			dc.queue.Forget(item)
		}
	}

	return true
}

func (dc *DynamicController) syncFunc(ctx context.Context, oi ObjectIdentifiers, handler Handler) error {
	gvrKey := keyFromGVR(oi.GVR)
	dc.log.V(1).Info("Syncing object", "gvr", gvrKey, "key", oi.NamespacedName)

	startTime := time.Now()
	defer func() {
		duration := time.Since(startTime)
		reconcileDuration.WithLabelValues(gvrKey).Observe(duration.Seconds())
		reconcileTotal.WithLabelValues(gvrKey).Inc()
		dc.log.V(1).Info("Finished syncing object",
			"gvr", gvrKey, "key", oi.NamespacedName, "duration", duration)
	}()

	err := handler(ctx, ctrl.Request{NamespacedName: oi.NamespacedName})
	if err != nil {
		handlerErrorsTotal.WithLabelValues(gvrKey).Inc()
	}
	return err
}

func (dc *DynamicController) enqueueParent(parentGVR schema.GroupVersionResource, obj interface{}, eventType string) {
	mobj, err := meta.Accessor(obj)
	if err != nil {
		dc.log.Error(err, "Failed to get meta for object to enqueue", "eventType", eventType)
		return
	}

	oi := ObjectIdentifiers{NamespacedName: types.NamespacedName{
		Namespace: mobj.GetNamespace(),
		Name:      mobj.GetName(),
	}, GVR: parentGVR}
	dc.log.V(1).Info("Enqueueing object", "objectIdentifiers", oi, "eventType", eventType)

	informerEventsTotal.WithLabelValues(parentGVR.String(), eventType).Inc()
	dc.queue.Add(oi)
}

func (dc *DynamicController) updateFunc(parentGVR schema.GroupVersionResource, oldObj, newObj interface{}) {
	newMeta, err := meta.Accessor(newObj)
	if err != nil {
		dc.log.Error(err, "failed to access new object meta")
		return
	}
	oldMeta, err := meta.Accessor(oldObj)
	if err != nil {
		dc.log.Error(err, "failed to access old object meta")
		return
	}
	if newMeta.GetGeneration() == oldMeta.GetGeneration() {
		dc.log.V(2).Info("Skipping update due to unchanged generation",
			"name", newMeta.GetName(), "namespace", newMeta.GetNamespace(), "generation", newMeta.GetGeneration())
		return
	}
	dc.enqueueParent(parentGVR, newObj, eventTypeUpdate)
}

// Register registers parent and children via reconciliation.
func (dc *DynamicController) Register(
	_ context.Context,
	parent schema.GroupVersionResource,
	instanceHandler Handler,
	resourceGVRsToWatch ...schema.GroupVersionResource,
) error {
	dc.mu.Lock()
	defer dc.mu.Unlock()

	reg, exists := dc.registrations[parent]
	if !exists {
		reg = &registration{
			parentGVR:       parent,
			childHandlerIDs: make(map[schema.GroupVersionResource]string),
		}
		dc.registrations[parent] = reg
	}

	if err := dc.reconcileParentLocked(parent, instanceHandler, reg); err != nil {
		return err
	}
	if err := dc.reconcileChildrenLocked(parent, resourceGVRsToWatch, reg); err != nil {
		return err
	}

	// kick reconciliation for existing parent objects
	if w, ok := dc.watches[parent]; ok && !w.Informer().IsStopped() {
		// Use informer cache if running to repopulate the queue.
		objects := w.Informer().GetStore().List()
		for _, obj := range objects {
			dc.enqueueParent(parent, obj, "update")
		}
	}

	dc.log.V(1).Info("Successfully registered GVR", "gvr", keyFromGVR(parent))
	return nil
}

// Deregister clears parent and children and drops any queued items for the parent GVR.
func (dc *DynamicController) Deregister(_ context.Context, parent schema.GroupVersionResource) error {
	dc.mu.Lock()
	defer dc.mu.Unlock()

	reg, exists := dc.registrations[parent]
	if !exists {
		return nil
	}

	gvrKey := keyFromGVR(parent)

	if err := dc.reconcileChildrenLocked(parent, nil, reg); err != nil {
		dc.log.Error(err, "failed to detach children", "parent", gvrKey)
	}
	if err := dc.reconcileParentLocked(parent, nil, reg); err != nil {
		dc.log.Error(err, "failed to detach parent", "parent", gvrKey)
	}

	delete(dc.registrations, parent)

	dc.log.V(1).Info("Successfully unregistered GVR", "gvr", gvrKey)
	return nil
}

// ----- internal helpers -----

func (dc *DynamicController) ensureWatchLocked(
	gvr schema.GroupVersionResource,
) *internal.LazyInformer {
	if w, ok := dc.watches[gvr]; ok {
		return w
	}

	// Create per-GVR watch wrapper (informer created lazily on first handler)
	w := internal.NewLazyInformer(dc.client, gvr, dc.config.ResyncPeriod, nil, dc.log)
	dc.watches[gvr] = w
	return w
}

// reconcileParentLocked ensures a parent watch exists and has exactly one handler.
// If instanceHandler is nil, the parent handler is removed.
// Must be called with dc.mu held.
func (dc *DynamicController) reconcileParentLocked(
	parent schema.GroupVersionResource,
	instanceHandler Handler,
	reg *registration,
) error {
	if instanceHandler == nil {
		if reg.parentHandlerID == "" {
			return nil
		}
		// remove parent handler if present
		if err := dc.removeHandlerLocked(parent, reg.parentHandlerID); err != nil {
			return fmt.Errorf("removing parent handler %s: %w", parent, err)
		}
		reg.parentHandlerID = ""
		dc.handlers.Delete(parent)

		gvrCount.Dec()
		handlerDetachTotal.WithLabelValues("parent").Inc()
		handlerCount.WithLabelValues("parent").Dec()
		dc.log.V(1).Info("Detached parent", "gvr", parent)
		return nil
	}

	// ensure watch
	w := dc.ensureWatchLocked(parent)

	// create handler if missing
	if reg.parentHandlerID == "" {
		parentHandlerID := parentHandlerID(parent)
		if err := w.AddHandler(dc.ctx, parentHandlerID, cache.ResourceEventHandlerFuncs{
			AddFunc:    func(obj interface{}) { dc.enqueueParent(parent, obj, eventTypeAdd) },
			UpdateFunc: func(oldObj, newObj interface{}) { dc.updateFunc(parent, oldObj, newObj) },
			DeleteFunc: func(obj interface{}) { dc.enqueueParent(parent, obj, eventTypeDelete) },
		}); err != nil {
			return fmt.Errorf("add parent handler %s: %w", parent, err)
		}
		reg.parentHandlerID = parentHandlerID

		gvrCount.Inc()
		handlerAttachTotal.WithLabelValues("parent").Inc()
		handlerCount.WithLabelValues("parent").Inc()
		dc.log.V(1).Info("Attached parent", "gvr", parent)
	}

	// always update latest handler
	dc.handlers.Store(parent, instanceHandler)

	return nil
}

// reconcileChildrenLocked ensures that reg.childHandlerIDs matches the desired set.
// Must be called with dc.mu held.
func (dc *DynamicController) reconcileChildrenLocked(
	parent schema.GroupVersionResource,
	desired []schema.GroupVersionResource,
	reg *registration,
) error {
	desiredSet := make(map[schema.GroupVersionResource]struct{}, len(desired))
	for _, g := range desired {
		desiredSet[g] = struct{}{}
	}

	parentGVRKey := keyFromGVR(parent)

	// remove obsolete
	for child, childHandlerID := range reg.childHandlerIDs {
		if _, keep := desiredSet[child]; keep {
			continue
		}
		if err := dc.removeHandlerLocked(child, childHandlerID); err != nil {
			return fmt.Errorf("removing child handler %s: %w", child, err)
		}
		delete(reg.childHandlerIDs, child)

		childGVRKey := keyFromGVR(child)
		handlerDetachTotal.WithLabelValues("child").Inc()
		handlerCount.WithLabelValues("child").Dec()
		dc.log.V(1).Info("Detached child", "parent", parentGVRKey, "gvr", childGVRKey)
	}

	// add missing
	for child := range desiredSet {
		if _, exists := reg.childHandlerIDs[child]; exists {
			continue
		}
		w := dc.ensureWatchLocked(child)
		childHandlerID := childHandlerID(parent, child)
		if err := w.AddHandler(dc.ctx, childHandlerID, dc.handlerForChildGVR(parent, child)); err != nil {
			return fmt.Errorf("add child handler %s: %w", child, err)
		}
		reg.childHandlerIDs[child] = childHandlerID

		childGVRKey := keyFromGVR(child)
		handlerAttachTotal.WithLabelValues("child").Inc()
		handlerCount.WithLabelValues("child").Inc()
		dc.log.V(1).Info("Attached child", "parent", parentGVRKey, "gvr", childGVRKey)
	}

	return nil
}

// removeHandlerLocked removes a handler with the specified ID for a given GroupVersionResource watch.
// If the handler removal stops the watch, the corresponding GroupVersionResource entry is deleted from the watches map.
// Returns an error if the handler removal fails.
func (dc *DynamicController) removeHandlerLocked(gvr schema.GroupVersionResource, handlerID string) error {
	w, ok := dc.watches[gvr]
	if !ok {
		// no watch means there cannot be a handler
		return nil
	}
	stopped, err := w.RemoveHandler(handlerID)
	if err != nil {
		return fmt.Errorf("removing handler %s for watch on %s failed: %w", handlerID, gvr, err)
	}
	if stopped {
		delete(dc.watches, gvr)
	}
	return nil
}

func (dc *DynamicController) handlerForChildGVR(parent, child schema.GroupVersionResource) cache.ResourceEventHandler {
	parentGVRKey, childGVRKey := keyFromGVR(parent), keyFromGVR(child)
	handle := func(obj interface{}, eventType string) {
		objMeta, err := meta.Accessor(obj)
		if err != nil {
			dc.log.Error(err, "failed to get metadata accessor for object", "eventType", eventType)
			return
		}
		lbls := objMeta.GetLabels()
		owned, ok := lbls[metadata.OwnedLabel]
		if !ok || owned != "true" {
			return
		}
		name, ok := lbls[metadata.InstanceLabel]
		if !ok {
			return
		}
		namespace, ok := lbls[metadata.InstanceNamespaceLabel]
		if !ok {
			return
		}

		parentGVK, err := dc.mapper.KindFor(parent)
		if err != nil {
			dc.log.Error(err, "failed to get parent GVK", "parent", parentGVRKey)
			return
		}
		pom := &metav1.PartialObjectMetadata{}
		pom.SetGroupVersionKind(parentGVK)
		pom.SetName(name)
		pom.SetNamespace(namespace)

		dc.log.V(1).Info("Child triggered parent reconciliation",
			"parent", parentGVRKey,
			"child", childGVRKey,
			"eventType", eventType,
			"childName", objMeta.GetName(),
			"childNamespace", objMeta.GetNamespace(),
			"targetName", name,
			"targetNamespace", namespace,
		)
		dc.enqueueParent(parent, pom, eventType)
	}
	return cache.ResourceEventHandlerFuncs{
		AddFunc:    func(obj interface{}) { handle(obj, eventTypeAdd) },
		UpdateFunc: func(oldObj, newObj interface{}) { handle(newObj, eventTypeUpdate) },
		DeleteFunc: func(obj interface{}) { handle(obj, eventTypeDelete) },
	}
}

func (dc *DynamicController) gracefulShutdown() error {
	dc.log.Info("Starting graceful shutdown")

	dc.mu.Lock()
	for gvr, w := range dc.watches {
		dc.log.V(1).Info("Stopping watch", "gvr", keyFromGVR(gvr))
		w.Shutdown()
	}
	dc.mu.Unlock()

	queueShutdownDone := make(chan struct{})
	go func() {
		dc.queue.ShutDown()
		close(queueShutdownDone)
	}()

	ctx := context.Background()
	var cancel context.CancelFunc
	if dc.config.QueueShutdownTimeout > 0 {
		ctx, cancel = context.WithTimeout(ctx, dc.config.QueueShutdownTimeout)
	} else {
		ctx, cancel = context.WithCancel(ctx)
	}
	defer cancel()

	select {
	case <-queueShutdownDone:
		return nil
	case <-ctx.Done():
		return fmt.Errorf("timeout waiting for queue to shutdown: %w", ctx.Err())
	}
}

// keyFromGVR returns a compact, allocation-efficient string key for the given
// GroupVersionResource in the canonical "group/version/resource" format.
// Unlike [schema.GroupVersionResource.String], it omits labels and avoids extra
// allocations, making it suitable for use as map keys, metrics labels, and cache identifiers.
func keyFromGVR(gvr schema.GroupVersionResource) string {
	var b strings.Builder
	if gvr.Group != "" {
		b.WriteString(gvr.Group)
	}
	if gvr.Version != "" {
		b.WriteRune('/')
		b.WriteString(gvr.Version)
	}
	if gvr.Resource != "" {
		b.WriteRune('/')
		b.WriteString(gvr.Resource)
	}
	return b.String()
}

func childHandlerID(parent schema.GroupVersionResource, child schema.GroupVersionResource) string {
	return "child:" + keyFromGVR(parent) + "->" + keyFromGVR(child)
}

func parentHandlerID(parent schema.GroupVersionResource) string {
	handlerID := "parent:" + keyFromGVR(parent)
	return handlerID
}
